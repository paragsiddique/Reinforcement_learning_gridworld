{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning in the gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that an agent is situated in the 4×3 environment shown in the figure below. Beginning\n",
    "in the start state, it must choose an action at each time step. The interaction with the environment\n",
    "terminates when the agent reaches one of the goal states, marked +1 or –1. Aavailable actions are Up, Down, Left, and Right. We assume, this gridworld is deterministic, meaning  the agent will go where it intends to go. For example, when the agent decides to take action up at (0, 1), it will land in (0, 2) rather than elsewhere. \n",
    "\n",
    "We apply reinforcement learning to find best traveling path for the agent. \n",
    "\n",
    "<img src='gridworld.png' height=\"40%\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encode the world as below:\n",
    "<img src='gridworld_coded.png'  height=\"40%\" width=\"42%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_POSSIBLE_ACTIONS = ['U', 'D', 'L', 'R']\n",
    "GOAL = (3, 2)\n",
    "PIT = (3, 1)\n",
    "WALL = (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOAL_REWARD = 1\n",
    "PIT_REWARD = -1\n",
    "WALL_REWARD = 0\n",
    "STANDARD_REWARD = -0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_states = [(0, 0), (0, 1), (0, 2),\\\n",
    "                  (1, 0), (1, 1), (1, 2),\\\n",
    "                  (2, 0), (2, 1), (2, 2),\\\n",
    "                  (3, 0), (3, 1), (3, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_dictionary = {(0, 0): 0, (0, 1): 0, (0, 2): 0,\\\n",
    "                    (1, 0): 0, (1, 1): 0, (1, 2): 0,\\\n",
    "                    (2, 0): 0, (2, 1): 0, (2, 2): 0,\\\n",
    "                    (3, 0): 0, (3, 1): 0, (3, 2): 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(state):\n",
    "    if state == GOAL:\n",
    "        return GOAL_REWARD\n",
    "    elif state == PIT:\n",
    "        return PIT_REWARD\n",
    "    elif state == WALL:\n",
    "        return WALL_REWARD\n",
    "    else:\n",
    "        return STANDARD_REWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_to_explore = [state for state in all_the_states\\\n",
    "                     if state != GOAL and state != PIT and state != WALL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_look_ahead(current_state, action, value_dictionary):\n",
    "    # output: action, state_prime, reward, value\n",
    "    x = current_state[0]\n",
    "    y = current_state[1]\n",
    "    \n",
    "\n",
    "    if action == 'U':\n",
    "        next_state = x, y + 1\n",
    "    elif action == 'R':\n",
    "        next_state = x + 1, y\n",
    "    elif action == 'D':\n",
    "        next_state = x, y - 1\n",
    "    else:\n",
    "        next_state = x - 1, y\n",
    "\n",
    "    (i, j) = next_state\n",
    "    if (i, j) != (1, 1) and ((0 <= i <= 3) and (0 <= j <= 2)):\n",
    "        return action, (i, j), compute_reward((i, j)), value_dictionary[(i, j)] \n",
    "    else:\n",
    "        return action, None, compute_reward((i, j)), 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_value_state(current_state, value_dictionary):\n",
    "    transitional_list = []\n",
    "    for action in ALL_POSSIBLE_ACTIONS:\n",
    "        transitional_list.append(one_step_look_ahead(current_state, action, value_dictionary)) \n",
    "\n",
    "    value_next_state_dict = {}\n",
    "    for x in transitional_list:\n",
    "        value_next_state_dict[x] = x[2] + GAMMA*x[3]\n",
    "    \n",
    "    max_value = max(value_next_state_dict.values()) \n",
    "\n",
    "    max_value_states = [k for k,v in value_next_state_dict.items() if v == max_value]\n",
    "    \n",
    "    if len(max_value_states) > 1:\n",
    "        max_value_state = random.choice(max_value_states)\n",
    "    else:\n",
    "        max_value_state = max_value_states[0]\n",
    "\n",
    "    return max_value_state, value_next_state_dict[max_value_state]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(states_to_explore, value_dictionary):\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in states_to_explore:\n",
    "            old_value = value_dictionary[state]\n",
    "            _, new_value = max_value_state(state, value_dictionary)\n",
    "            value_dictionary[state] = new_value\n",
    "            delta = max(delta, abs(new_value - old_value))\n",
    "        if delta < THRESHOLD:\n",
    "            break\n",
    "    return value_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_value(value_dictionary):\n",
    "    #print('\\n')\n",
    "    print(\"      0      1      2      3  \")\n",
    "    print(\"  +------+------+------+------+\")\n",
    "    print(\"2 | %.2f | %.2f | %.2f | %.2f |\" %(value_dictionary[(0, 2)], value_dictionary[(1, 2)], value_dictionary[(2, 2)], value_dictionary[(3, 2)]))\n",
    "    print(\"  +------+------+------+------+\")\n",
    "    print(\"1 | %.2f | %.2f | %.2f | %.2f |\" %(value_dictionary[(0, 1)], value_dictionary[(1, 1)], value_dictionary[(2, 1)], value_dictionary[(3, 1)]))\n",
    "    print(\"  +------+------+------+------+\")\n",
    "    print(\"0 | %.2f | %.2f | %.2f | %.2f |\" %(value_dictionary[(0, 0)], value_dictionary[(1, 0)], value_dictionary[(2, 0)], value_dictionary[(3, 0)]))\n",
    "    print(\"  +------+------+------+------+\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_extraction(states_to_explore, value_dictionary):\n",
    "    policy_dictionary = {}\n",
    "    for state in states_to_explore:\n",
    "        policy_dictionary[state] = max_value_state(state, value_dictionary)[0][0]\n",
    "    return policy_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_policy(policy_dictionary):\n",
    "    #print('G: poal, P: pit, W: wall')\n",
    "    print(\"     0     1     2     3  \")\n",
    "    print(\"  +-----+-----+-----+-----+\")\n",
    "    print(\"2 |  %s  |  %s  |  %s  |     |\" %(policy_dictionary[(0, 2)], policy_dictionary[(1, 2)], policy_dictionary[(2, 2)]))\n",
    "    print(\"  +-----+-----+-----+-----+\")\n",
    "    print(\"1 |  %s  |     |  %s  |     |\" %(policy_dictionary[(0, 1)], policy_dictionary[(2, 1)]))\n",
    "    print(\"  +-----+-----+-----+-----+\")\n",
    "    print(\"0 |  %s  |  %s  |  %s  |  %s  |\" %(policy_dictionary[(0, 0)], policy_dictionary[(1, 0)], policy_dictionary[(2, 0)], policy_dictionary[(3, 0)]))\n",
    "    print(\"  +-----+-----+-----+-----+\")\n",
    "    #print('\\nG: poal, P: pit, W: wall')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "THRESHOLD = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0      1      2      3  \n",
      "  +------+------+------+------+\n",
      "2 | 0.73 | 0.86 | 1.00 | 0.00 |\n",
      "  +------+------+------+------+\n",
      "1 | 0.62 | 0.00 | 0.86 | 0.00 |\n",
      "  +------+------+------+------+\n",
      "0 | 0.52 | 0.62 | 0.73 | 0.62 |\n",
      "  +------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "value_iterate = value_iteration(states_to_explore, value_dictionary)\n",
    "display_value(value_iterate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0     1     2     3  \n",
      "  +-----+-----+-----+-----+\n",
      "2 |  R  |  R  |  R  |     |\n",
      "  +-----+-----+-----+-----+\n",
      "1 |  U  |     |  U  |     |\n",
      "  +-----+-----+-----+-----+\n",
      "0 |  U  |  R  |  U  |  L  |\n",
      "  +-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_policy(policy_extraction(states_to_explore, value_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=14BfO5lMiuk&list=PLWzQK00nc192L7UMJyTmLXaHa3KcO0wBT&index=1\n",
    "\n",
    "https://github.com/colinskow/move37/tree/master/dynamic_programming\n",
    "\n",
    "https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
